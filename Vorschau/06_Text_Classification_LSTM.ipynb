{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# Text classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUWearf0Gw-p"
   },
   "source": [
    "This text classification tutorial trains a recurrent neural network on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2VQo4bajwUU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (4.9.7)\n",
      "Requirement already satisfied: absl-py in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (8.1.8)\n",
      "Requirement already satisfied: dm-tree in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: immutabledict in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (1.24.3)\n",
      "Requirement already satisfied: promise in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (6.1.1)\n",
      "Requirement already satisfied: pyarrow in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (19.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (2.5.0)\n",
      "Requirement already satisfied: toml in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: etils>=1.6.0 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.12.0)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from dm-tree->tensorflow_datasets) (25.1.0)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/miniconda3/envs/gfu_s1912/lib/python3.10/site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "## Setup input pipeline\n",
    "\n",
    "\n",
    "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
    "\n",
    "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SHRwRoP2nVHX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 15:59:58.005261: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/phbo/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/phbo/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWA4c2ir7g6p"
   },
   "source": [
    "Initially this returns a dataset of (text, label pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vd4_BGKyurao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 16:01:19.658986: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2qVJzcEluH_"
   },
   "source": [
    "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dDsCaZCDYZgm"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VznrltNOnUc5"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jqkvdcFv41wC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"I'm sorry, I had high hopes for this movie. Unfortunately, it was too long, too thin and too weak to hold my attention. When I realized the whole movie was indeed only about an older guy reliving his dream, I felt cheated. Surely it could have been a device to bring us into something deeper, something more meaningful.<br /><br />So, don't buy a large drink or you'll be running to the rest room. My kids didn't enjoy it either. Ah well.\"\n",
      "\n",
      "label:  0\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "text:  b\"The memory banks of most of the reviewers here must've short-circuited when trying to recall this Cubic Zirconia of a gem, because practically everyone managed to misquote Lloyd Bochner's Walter Thornton, when in a fit of peevish anger, he hurls the phallic garden nozzle at his new wife, Jerilee Randall-Thornton, (a nearly comatose Pia Zadora) which was used to sexually assault her earlier in the movie...but I'm getting ahead of myself. In any case, poor Lloyd could've been snarling that line at the speechless audience as much as he was his put-upon co-star.<br /><br />Hard as it is for most of us to believe, especially these days, nobody in Hollywood sets out to INTENTIONALLY make a bad movie. This is certainly not the most defensible argument to make, since there just seem to be so damn many of them coming out. But then again, there is that breed of film that one must imagine during the time of its creation, from writing, casting and direction, must've been cursed with the cinematic equivalent of trying to shoot during the Ides of March.<br /><br />THE LONELY LADY is in that category, and represents itself very well, considering the circumstances. Here we have all the ingredients in a recipe guaranteed to produce a monumentally fallen souffl\\xc3\\xa9: Pia Zadora, a marginal singer/actress so determined to be taken seriously, that she would take on practically anything that might set her apart from her peers, (which this movie most certainly did!); a somewhat high-profile novel written by the Trashmaster himself, Harold Robbins (of THE CARPETBAGGERS and DREAMS DIE FIRST fame); a cast who probably thought they were so fortunate to be working at all, that they tried to play this dreck like it was Clifford Odets or Ibsen; plus a director who more than likely was a hired gun who kept the mess moving just to collect a paycheck, (and was probably contractually obligated NOT to demand the use of the 'Alan Smithee' moniker to protect what was left of his reputation.) Like Lamont Johnson's LIPSTICK, Meir Zarchi's I SPIT ON YOUR GRAVE, Roger Vadim's BARBARELLA, Paul Verhoeven's SHOWGIRLS or the Grandmammy of Really Bad Film-making, Frank Perry's MOMMY DEAREST, THE LONELY LADY is still often-discussed, (usually with disgust, disbelief, horrified laughter, or a unique combination of all three), yet also defies dissection, description or even the pretzel logic of Hollyweird. Nobody's sure how it came to be, how it was ever released in even a single theater, or why it's still here and nearly impossible to get rid of, but take it or leave it, it IS here to stay. And I don't think that lovers of really good BAD movies would have it any other way.\"\n",
      "\n",
      "label:  0\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "text:  b'I remember watching this in the 1970s - then I have just recently borrowed a couple of episodes from our public library.<br /><br />With a nearly 30 year hiatus, I have come to another conclusion. Most of the principals interviewed in this series - some at the center of power like Traudl Junge (Hitler\\'s Secretary),Karl Doenitz (head of Germany\\'s navy) Anthony Eden (UK) - are long gone but their first hand accounts will live on.From Generals and Admirals to Sergeants, Russian civilians, concentration camp survivors, all are on record here. <br /><br />I can remember the Lord Mountbatten interview (killed in the 1970s) <br /><br />This is truly a gem and I believe the producer of this series was knighted by Queen Elizabeth for this work - well deserved.<br /><br />Seeing these few episodes from the library makes me want to buy the set.<br /><br />This is the only \"10\" I have given any review but I have discovered like a fine bottle of wine, it is more appreciated with a little time...'\n",
      "\n",
      "label:  1\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 16:01:19.814774: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, example_label in train_dataset.take(1):\n",
    "    for i in range(3):\n",
    "        print('text: ', example.numpy()[i])\n",
    "        print()\n",
    "        print('label: ', example_label.numpy()[i])\n",
    "        print('--------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5eWCo88voPY"
   },
   "source": [
    "## Create the text encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFevcItw15P_"
   },
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
    "\n",
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uC25Lu1Yvuqy"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuQzVBbe3Ldu"
   },
   "source": [
    "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tBoyjjWg0Ac9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
       "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjId5pua3jHQ"
   },
   "source": [
    "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RGc7C9WiwRWs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142, 786,  10, ...,   0,   0,   0],\n",
       "       [  2,   1,   1, ...,   0,   0,   0],\n",
       "       [ 10, 364, 147, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example)[:3].numpy()\n",
    "encoded_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b\"I'm sorry, I had high hopes for this movie. Unfortunately, it was too long, too thin and too weak to hold my attention. When I realized the whole movie was indeed only about an older guy reliving his dream, I felt cheated. Surely it could have been a device to bring us into something deeper, something more meaningful.<br /><br />So, don't buy a large drink or you'll be running to the rest room. My kids didn't enjoy it either. Ah well.\",\n",
       "       b\"The memory banks of most of the reviewers here must've short-circuited when trying to recall this Cubic Zirconia of a gem, because practically everyone managed to misquote Lloyd Bochner's Walter Thornton, when in a fit of peevish anger, he hurls the phallic garden nozzle at his new wife, Jerilee Randall-Thornton, (a nearly comatose Pia Zadora) which was used to sexually assault her earlier in the movie...but I'm getting ahead of myself. In any case, poor Lloyd could've been snarling that line at the speechless audience as much as he was his put-upon co-star.<br /><br />Hard as it is for most of us to believe, especially these days, nobody in Hollywood sets out to INTENTIONALLY make a bad movie. This is certainly not the most defensible argument to make, since there just seem to be so damn many of them coming out. But then again, there is that breed of film that one must imagine during the time of its creation, from writing, casting and direction, must've been cursed with the cinematic equivalent of trying to shoot during the Ides of March.<br /><br />THE LONELY LADY is in that category, and represents itself very well, considering the circumstances. Here we have all the ingredients in a recipe guaranteed to produce a monumentally fallen souffl\\xc3\\xa9: Pia Zadora, a marginal singer/actress so determined to be taken seriously, that she would take on practically anything that might set her apart from her peers, (which this movie most certainly did!); a somewhat high-profile novel written by the Trashmaster himself, Harold Robbins (of THE CARPETBAGGERS and DREAMS DIE FIRST fame); a cast who probably thought they were so fortunate to be working at all, that they tried to play this dreck like it was Clifford Odets or Ibsen; plus a director who more than likely was a hired gun who kept the mess moving just to collect a paycheck, (and was probably contractually obligated NOT to demand the use of the 'Alan Smithee' moniker to protect what was left of his reputation.) Like Lamont Johnson's LIPSTICK, Meir Zarchi's I SPIT ON YOUR GRAVE, Roger Vadim's BARBARELLA, Paul Verhoeven's SHOWGIRLS or the Grandmammy of Really Bad Film-making, Frank Perry's MOMMY DEAREST, THE LONELY LADY is still often-discussed, (usually with disgust, disbelief, horrified laughter, or a unique combination of all three), yet also defies dissection, description or even the pretzel logic of Hollyweird. Nobody's sure how it came to be, how it was ever released in even a single theater, or why it's still here and nearly impossible to get rid of, but take it or leave it, it IS here to stay. And I don't think that lovers of really good BAD movies would have it any other way.\",\n",
       "       b'I remember watching this in the 1970s - then I have just recently borrowed a couple of episodes from our public library.<br /><br />With a nearly 30 year hiatus, I have come to another conclusion. Most of the principals interviewed in this series - some at the center of power like Traudl Junge (Hitler\\'s Secretary),Karl Doenitz (head of Germany\\'s navy) Anthony Eden (UK) - are long gone but their first hand accounts will live on.From Generals and Admirals to Sergeants, Russian civilians, concentration camp survivors, all are on record here. <br /><br />I can remember the Lord Mountbatten interview (killed in the 1970s) <br /><br />This is truly a gem and I believe the producer of this series was knighted by Queen Elizabeth for this work - well deserved.<br /><br />Seeing these few episodes from the library makes me want to buy the set.<br /><br />This is the only \"10\" I have given any review but I have discovered like a fine bottle of wine, it is more appreciated with a little time...'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5cjz0bS39IN"
   },
   "source": [
    "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
    "\n",
    "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
    "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N_tD0QY5wXaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  b\"I'm sorry, I had high hopes for this movie. Unfortunately, it was too long, too thin and too weak to hold my attention. When I realized the whole movie was indeed only about an older guy reliving his dream, I felt cheated. Surely it could have been a device to bring us into something deeper, something more meaningful.<br /><br />So, don't buy a large drink or you'll be running to the rest room. My kids didn't enjoy it either. Ah well.\"\n",
      "\n",
      "Round-trip:  im sorry i had high [UNK] for this movie unfortunately it was too long too [UNK] and too weak to [UNK] my attention when i [UNK] the whole movie was indeed only about an older guy [UNK] his dream i felt [UNK] [UNK] it could have been a [UNK] to bring us into something [UNK] something more [UNK] br so dont buy a [UNK] [UNK] or youll be running to the rest room my kids didnt enjoy it either [UNK] well                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Original:  b\"The memory banks of most of the reviewers here must've short-circuited when trying to recall this Cubic Zirconia of a gem, because practically everyone managed to misquote Lloyd Bochner's Walter Thornton, when in a fit of peevish anger, he hurls the phallic garden nozzle at his new wife, Jerilee Randall-Thornton, (a nearly comatose Pia Zadora) which was used to sexually assault her earlier in the movie...but I'm getting ahead of myself. In any case, poor Lloyd could've been snarling that line at the speechless audience as much as he was his put-upon co-star.<br /><br />Hard as it is for most of us to believe, especially these days, nobody in Hollywood sets out to INTENTIONALLY make a bad movie. This is certainly not the most defensible argument to make, since there just seem to be so damn many of them coming out. But then again, there is that breed of film that one must imagine during the time of its creation, from writing, casting and direction, must've been cursed with the cinematic equivalent of trying to shoot during the Ides of March.<br /><br />THE LONELY LADY is in that category, and represents itself very well, considering the circumstances. Here we have all the ingredients in a recipe guaranteed to produce a monumentally fallen souffl\\xc3\\xa9: Pia Zadora, a marginal singer/actress so determined to be taken seriously, that she would take on practically anything that might set her apart from her peers, (which this movie most certainly did!); a somewhat high-profile novel written by the Trashmaster himself, Harold Robbins (of THE CARPETBAGGERS and DREAMS DIE FIRST fame); a cast who probably thought they were so fortunate to be working at all, that they tried to play this dreck like it was Clifford Odets or Ibsen; plus a director who more than likely was a hired gun who kept the mess moving just to collect a paycheck, (and was probably contractually obligated NOT to demand the use of the 'Alan Smithee' moniker to protect what was left of his reputation.) Like Lamont Johnson's LIPSTICK, Meir Zarchi's I SPIT ON YOUR GRAVE, Roger Vadim's BARBARELLA, Paul Verhoeven's SHOWGIRLS or the Grandmammy of Really Bad Film-making, Frank Perry's MOMMY DEAREST, THE LONELY LADY is still often-discussed, (usually with disgust, disbelief, horrified laughter, or a unique combination of all three), yet also defies dissection, description or even the pretzel logic of Hollyweird. Nobody's sure how it came to be, how it was ever released in even a single theater, or why it's still here and nearly impossible to get rid of, but take it or leave it, it IS here to stay. And I don't think that lovers of really good BAD movies would have it any other way.\"\n",
      "\n",
      "Round-trip:  the [UNK] [UNK] of most of the [UNK] here [UNK] [UNK] when trying to [UNK] this [UNK] [UNK] of a [UNK] because [UNK] everyone [UNK] to [UNK] [UNK] [UNK] [UNK] [UNK] when in a [UNK] of [UNK] [UNK] he [UNK] the [UNK] [UNK] [UNK] at his new wife [UNK] [UNK] a nearly [UNK] [UNK] [UNK] which was used to [UNK] [UNK] her earlier in the [UNK] im getting [UNK] of myself in any case poor [UNK] [UNK] been [UNK] that line at the [UNK] audience as much as he was his [UNK] [UNK] br hard as it is for most of us to believe especially these days [UNK] in hollywood sets out to [UNK] make a bad movie this is certainly not the most [UNK] [UNK] to make since there just seem to be so [UNK] many of them coming out but then again there is that [UNK] of film that one must imagine during the time of its [UNK] from writing casting and direction [UNK] been [UNK] with the [UNK] [UNK] of trying to [UNK] during the [UNK] of [UNK] br the [UNK] lady is in that [UNK] and [UNK] itself very well [UNK] the [UNK] here we have all the [UNK] in a [UNK] [UNK] to [UNK] a [UNK] [UNK] [UNK] [UNK] [UNK] a [UNK] [UNK] so [UNK] to be taken seriously that she would take on [UNK] anything that might set her apart from her [UNK] which this movie most certainly did a somewhat [UNK] novel written by the [UNK] himself [UNK] [UNK] of the [UNK] and [UNK] die first [UNK] a cast who probably thought they were so [UNK] to be working at all that they tried to play this [UNK] like it was [UNK] [UNK] or [UNK] plus a director who more than [UNK] was a [UNK] [UNK] who kept the mess moving just to [UNK] a [UNK] and was probably [UNK] [UNK] not to [UNK] the use of the [UNK] [UNK] [UNK] to [UNK] what was left of his [UNK] like [UNK] [UNK] [UNK] [UNK] [UNK] i [UNK] on your [UNK] [UNK] [UNK] [UNK] paul [UNK] [UNK] or the [UNK] of really bad [UNK] [UNK] [UNK] [UNK] [UNK] the [UNK] lady is still [UNK] usually with [UNK] [UNK] [UNK] [UNK] or a unique [UNK] of all three yet also [UNK] [UNK] [UNK] or even the [UNK] [UNK] of [UNK] [UNK] sure how it came to be how it was ever released in even a single theater or why its still here and nearly [UNK] to get [UNK] of but take it or leave it it is here to stay and i dont think that [UNK] of really good bad movies would have it any other way                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Original:  b'I remember watching this in the 1970s - then I have just recently borrowed a couple of episodes from our public library.<br /><br />With a nearly 30 year hiatus, I have come to another conclusion. Most of the principals interviewed in this series - some at the center of power like Traudl Junge (Hitler\\'s Secretary),Karl Doenitz (head of Germany\\'s navy) Anthony Eden (UK) - are long gone but their first hand accounts will live on.From Generals and Admirals to Sergeants, Russian civilians, concentration camp survivors, all are on record here. <br /><br />I can remember the Lord Mountbatten interview (killed in the 1970s) <br /><br />This is truly a gem and I believe the producer of this series was knighted by Queen Elizabeth for this work - well deserved.<br /><br />Seeing these few episodes from the library makes me want to buy the set.<br /><br />This is the only \"10\" I have given any review but I have discovered like a fine bottle of wine, it is more appreciated with a little time...'\n",
      "\n",
      "Round-trip:  i remember watching this in the [UNK] then i have just [UNK] [UNK] a couple of episodes from our [UNK] [UNK] br with a nearly [UNK] year [UNK] i have come to another [UNK] most of the [UNK] [UNK] in this series some at the [UNK] of power like [UNK] [UNK] [UNK] [UNK] [UNK] head of [UNK] [UNK] [UNK] [UNK] [UNK] are long gone but their first hand [UNK] will live [UNK] [UNK] and [UNK] to [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] all are on [UNK] here br br i can remember the [UNK] [UNK] [UNK] killed in the [UNK] br br this is truly a [UNK] and i believe the [UNK] of this series was [UNK] by [UNK] [UNK] for this work well [UNK] br seeing these few episodes from the [UNK] makes me want to buy the [UNK] br this is the only 10 i have given any review but i have [UNK] like a fine [UNK] of [UNK] it is more [UNK] with a little time                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(3):\n",
    "  print(\"Original: \", example[n].numpy())\n",
    "  print()\n",
    "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
    "  print('----------------------------------------------------------------------------------------------------')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "W7zsmInBOCPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 0: syntax error near unexpected token `('\n",
      "/bin/bash: -c: line 0: `[A drawing of the information flow in the model](../Bilder/bidirectional.jpg)'\n"
     ]
    }
   ],
   "source": [
    "![A drawing of the information flow in the model](../Bilder/bidirectional.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgs6nnSTGw-t"
   },
   "source": [
    "Above is a diagram of the model. \n",
    "\n",
    "1. This model can be build as a `tf.keras.Sequential`.\n",
    "\n",
    "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
    "\n",
    "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
    "\n",
    "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
    "\n",
    "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
    "\n",
    "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
    "\n",
    "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
    "\n",
    "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
    "\n",
    "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4fodCI7soQi"
   },
   "source": [
    "The code to implement this is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:00.954934Z",
     "iopub.status.busy": "2022-04-05T11:09:00.954389Z",
     "iopub.status.idle": "2022-04-05T11:09:02.517462Z",
     "shell.execute_reply": "2022-04-05T11:09:02.516860Z"
    },
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE,\n",
    "                              output_dim=64,\n",
    "                              # Use masking to handle the variable sequence lengths\n",
    "                              mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF-PsCk1LwjY"
   },
   "source": [
    "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:02.521653Z",
     "iopub.status.busy": "2022-04-05T11:09:02.521443Z",
     "iopub.status.idle": "2022-04-05T11:09:02.525737Z",
     "shell.execute_reply": "2022-04-05T11:09:02.525074Z"
    },
    "id": "87a8-CwfKebw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlS0iaUIWLpI"
   },
   "source": [
    "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:02.528931Z",
     "iopub.status.busy": "2022-04-05T11:09:02.528746Z",
     "iopub.status.idle": "2022-04-05T11:09:08.599833Z",
     "shell.execute_reply": "2022-04-05T11:09:08.598929Z"
    },
    "id": "O41gw3KfWHus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n",
      "[0.00292506]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0VQmGnEWcuz"
   },
   "source": [
    "Now, evaluate it again in a batch that additionally contains a longer sentence. The result should be identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:08.603723Z",
     "iopub.status.busy": "2022-04-05T11:09:08.603135Z",
     "iopub.status.idle": "2022-04-05T11:09:08.769155Z",
     "shell.execute_reply": "2022-04-05T11:09:08.768473Z"
    },
    "id": "UIgpuTeFNDzq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 268ms/step\n",
      "[0.00292506]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text with padding\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRI776ZcH3Tf"
   },
   "source": [
    "Compile the Keras model to configure the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:08.772981Z",
     "iopub.status.busy": "2022-04-05T11:09:08.772575Z",
     "iopub.status.idle": "2022-04-05T11:09:08.785616Z",
     "shell.execute_reply": "2022-04-05T11:09:08.784770Z"
    },
    "id": "kj2xei41YZjC"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwH3nto596k"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:09:08.789543Z",
     "iopub.status.busy": "2022-04-05T11:09:08.789017Z",
     "iopub.status.idle": "2022-04-05T11:14:26.672101Z",
     "shell.execute_reply": "2022-04-05T11:14:26.671445Z"
    },
    "id": "hw86wWS4YgR2"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:26.675786Z",
     "iopub.status.busy": "2022-04-05T11:14:26.675392Z",
     "iopub.status.idle": "2022-04-05T11:14:41.004483Z",
     "shell.execute_reply": "2022-04-05T11:14:41.003831Z"
    },
    "id": "BaNbXi43YgUT"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:41.007809Z",
     "iopub.status.busy": "2022-04-05T11:14:41.007366Z",
     "iopub.status.idle": "2022-04-05T11:14:41.306819Z",
     "shell.execute_reply": "2022-04-05T11:14:41.306126Z"
    },
    "id": "OZmwt_mzaQJk"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwSE_386uhxD"
   },
   "source": [
    "Run a prediction on a new sentence:\n",
    "\n",
    "If the prediction is >= 0.0, it is positive else it is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:41.310342Z",
     "iopub.status.busy": "2022-04-05T11:14:41.309922Z",
     "iopub.status.idle": "2022-04-05T11:14:43.743957Z",
     "shell.execute_reply": "2022-04-05T11:14:43.743292Z"
    },
    "id": "ZXgfQSgRW6zU"
   },
   "outputs": [],
   "source": [
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g1evcaRpTKm"
   },
   "source": [
    "## Stack two or more LSTM layers\n",
    "\n",
    "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
    "\n",
    "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
    "\n",
    "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
    "\n",
    "Here is what the flow of information looks like with `return_sequences=True`:\n",
    "\n",
    "![layered_bidirectional](../Bilder/layered_bidirectional.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbSClCrG1z8l"
   },
   "source": [
    "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:43.748032Z",
     "iopub.status.busy": "2022-04-05T11:14:43.747603Z",
     "iopub.status.idle": "2022-04-05T11:14:46.598105Z",
     "shell.execute_reply": "2022-04-05T11:14:46.597461Z"
    },
    "id": "jo1jjO3vn0jo"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:46.602098Z",
     "iopub.status.busy": "2022-04-05T11:14:46.601552Z",
     "iopub.status.idle": "2022-04-05T11:14:46.609032Z",
     "shell.execute_reply": "2022-04-05T11:14:46.608535Z"
    },
    "id": "hEPV5jVGp-is"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:14:46.612025Z",
     "iopub.status.busy": "2022-04-05T11:14:46.611535Z",
     "iopub.status.idle": "2022-04-05T11:23:47.832110Z",
     "shell.execute_reply": "2022-04-05T11:23:47.831373Z"
    },
    "id": "LeSE-YjdqAeN"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:23:47.835924Z",
     "iopub.status.busy": "2022-04-05T11:23:47.835438Z",
     "iopub.status.idle": "2022-04-05T11:24:12.097443Z",
     "shell.execute_reply": "2022-04-05T11:24:12.096771Z"
    },
    "id": "_LdwilM1qPM3"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:24:12.100809Z",
     "iopub.status.busy": "2022-04-05T11:24:12.100224Z",
     "iopub.status.idle": "2022-04-05T11:24:16.419040Z",
     "shell.execute_reply": "2022-04-05T11:24:16.418327Z"
    },
    "id": "ykUKnAoqbycW"
   },
   "outputs": [],
   "source": [
    "# predict on a sample text without padding.\n",
    "\n",
    "sample_text = ('The movie was not good. The animation and the graphics '\n",
    "               'were terrible. I would not recommend this movie.')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T11:24:16.422721Z",
     "iopub.status.busy": "2022-04-05T11:24:16.422130Z",
     "iopub.status.idle": "2022-04-05T11:24:16.650288Z",
     "shell.execute_reply": "2022-04-05T11:24:16.649603Z"
    },
    "id": "_YYub0EDtwCu"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_graphs(history, 'accuracy')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xvpE3BaGw_V"
   },
   "source": [
    "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
    "\n",
    "If you're interested in building custom RNNs, see the [Keras RNN Guide](https://www.tensorflow.org/guide/keras/rnn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4n9TsbGw-f"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-04-05T11:08:43.149995Z",
     "iopub.status.busy": "2022-04-05T11:08:43.149720Z",
     "iopub.status.idle": "2022-04-05T11:08:43.154347Z",
     "shell.execute_reply": "2022-04-05T11:08:43.153660Z"
    },
    "id": "0nbI5DtDGw-i"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
